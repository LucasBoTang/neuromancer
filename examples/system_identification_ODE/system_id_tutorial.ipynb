{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuroMANCER: System ID Tutorial\n",
    "\n",
    "This notebook outlines the basic structure of a training script for system identification with deep state space models.\n",
    "\n",
    "We begin by importing the necessary components from NeuroMANCER, as well as specifying some hyperparameters used to define the structure and optimization of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tuor369/anaconda3/envs/neuromancer/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import slim\n",
    "import psl\n",
    "\n",
    "from neuromancer import blocks, estimators, dynamics, arg\n",
    "from neuromancer.activations import activations\n",
    "from neuromancer.visuals import VisualizerOpen\n",
    "from neuromancer.trainer import Trainer\n",
    "from neuromancer.problem import Problem\n",
    "from neuromancer.constraint import Objective\n",
    "from neuromancer.simulators import OpenLoopSimulator, MultiSequenceOpenLoopSimulator\n",
    "from neuromancer.callbacks import SysIDCallback\n",
    "from neuromancer.loggers import BasicLogger, MLFlowLogger\n",
    "from neuromancer.dataset import read_file, normalize_data, split_sequence_data, SequenceDataset\n",
    "from neuromancer.constraint import Variable\n",
    "\n",
    "\n",
    "# params for configuration\n",
    "params = {\n",
    "    \"nsteps\": 32,             # Prediction horizon length.\n",
    "    \"nsim\": 10000,            # Number of temporal datapoints in the simulated dataset.\n",
    "    \"data_seed\": 408,         # Random seed for the dataset emulator.\n",
    "    \"norm\": [\"U\", \"D\", \"Y\"],  # Norms imposed on raw variables. U: inputs, D: disturbances, Y: outputs.\n",
    "    \"Q_y\": 1.0,               # Output tracking penalty weight.\n",
    "    \"Q_e\": 1.0,               # State estimator hidden prediction penalty weight.\n",
    "    \"Q_sub\": 1.0,             # Linear maps regularization weight.\n",
    "    \"Q_con_x\": 1.0,           # Hidden state constraints penalty weight.\n",
    "    \"Q_dx\": 0.1,              # Penalty weight on hidden state difference in one time step. \n",
    "    \"savedir\": \"test\",        # Where should your trained model and plots be saved (temp).\n",
    "    \"eval_metric\": \"loop_dev_ref_loss\",   # Metric for model selection and early stopping.\n",
    "    \"metrics\": [\"nstep_dev_loss\", \"loop_dev_loss\", \"best_loop_dev_loss\",\n",
    "               \"nstep_dev_ref_loss\", \"loop_dev_ref_loss\"],   # Metrics to be logged.\n",
    "    \"epochs\": 300,            # Number of training epochs.\n",
    "    \"lr\": 0.001,              # Learning rate.\n",
    "    \"patience\": 100,          # Early stopping patience.\n",
    "    \"warmup\": 10,             # Early stopping warmup.\n",
    "}\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "NeuroMANCER currently supports both static and emulated datasets (generated from governing equations for various systems) via [PSL](https://github.com/pnnl/psl). Here, we define get_sequence_dataloaders() function for generating dataloader from the raw  data given as dictionary.  \n",
    "For more details see datasets tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_dataloaders(\n",
    "    data, nsteps, moving_horizon=False, norm_type=\"zero-one\", split_ratio=None, num_workers=0,\n",
    "):\n",
    "    \"\"\"This will generate dataloaders and open-loop sequence dictionaries for a given dictionary of\n",
    "    data. Dataloaders are hard-coded for full-batch training to match NeuroMANCER's original\n",
    "    training setup.\n",
    "\n",
    "    :param data: (dict str: np.array or list[dict str: np.array]) data dictionary or list of data\n",
    "        dictionaries; if latter is provided, multi-sequence datasets are created and splits are\n",
    "        computed over the number of sequences rather than their lengths.\n",
    "    :param nsteps: (int) length of windowed subsequences for N-step training.\n",
    "    :param moving_horizon: (bool) whether to use moving horizon batching.\n",
    "    :param norm_type: (str) type of normalization; see function `normalize_data` for more info.\n",
    "    :param split_ratio: (list float) percentage of data in train and development splits; see\n",
    "        function `split_sequence_data` for more info.\n",
    "    \"\"\"\n",
    "\n",
    "    data, _ = normalize_data(data, norm_type)\n",
    "    train_data, dev_data, test_data = split_sequence_data(data, nsteps, moving_horizon, split_ratio)\n",
    "\n",
    "    train_data = SequenceDataset(\n",
    "        train_data,\n",
    "        nsteps=nsteps,\n",
    "        moving_horizon=moving_horizon,\n",
    "        name=\"train\",\n",
    "    )\n",
    "    dev_data = SequenceDataset(\n",
    "        dev_data,\n",
    "        nsteps=nsteps,\n",
    "        moving_horizon=moving_horizon,\n",
    "        name=\"dev\",\n",
    "    )\n",
    "    test_data = SequenceDataset(\n",
    "        test_data,\n",
    "        nsteps=nsteps,\n",
    "        moving_horizon=moving_horizon,\n",
    "        name=\"test\",\n",
    "    )\n",
    "\n",
    "    train_loop = train_data.get_full_sequence()\n",
    "    dev_loop = dev_data.get_full_sequence()\n",
    "    test_loop = test_data.get_full_sequence()\n",
    "\n",
    "    train_data = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=len(train_data),\n",
    "        shuffle=False,\n",
    "        collate_fn=train_data.collate_fn,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    dev_data = DataLoader(\n",
    "        dev_data,\n",
    "        batch_size=len(dev_data),\n",
    "        shuffle=False,\n",
    "        collate_fn=dev_data.collate_fn,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    test_data = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=len(test_data),\n",
    "        shuffle=False,\n",
    "        collate_fn=test_data.collate_fn,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return (train_data, dev_data, test_data), (train_loop, dev_loop, test_loop), train_data.dataset.dims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dictionary dataset with raw data\n",
    "# for available systems in PSL library check: psl.systems.keys()\n",
    "# for available datasets in PSL library check: psl.datasets.keys()\n",
    "\n",
    "system = 'aero'         # keyword of selected system\n",
    "\n",
    "#  load raw dataset\n",
    "if system in psl.emulators:\n",
    "    data = psl.emulators[system](nsim=nsim, ninit=0, seed=args.data_seed).simulate()\n",
    "elif system in psl.datasets:\n",
    "    data = read_file(psl.datasets[system])\n",
    "else:\n",
    "    data = read_file(system)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processed dataset format:  \n",
    "nsim = number of time steps in the dataset time series  \n",
    "nsteps = legth of the prediction horizon  \n",
    "Y = observed outputs  \n",
    "U = inputs  \n",
    "D = disturbances  \n",
    "Yp = past trajectories generated as Y[0:-nsteps]  \n",
    "Yf = future trajectories generated as Y[nesteps:]  \n",
    "data format in dataset dictionaries: train_data['key']: torch.Size([batch size (prediction horizon), number of batches, number of variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Train, Development, Test sets - nstep and loop format\n",
    "nstep_data, loop_data, dims = get_sequence_dataloaders(data, params['nsteps'])\n",
    "train_data, dev_data, test_data = nstep_data\n",
    "train_loop, dev_loop, test_loop = loop_data\n",
    "\n",
    "# input, output dimensions\n",
    "nu = dims['U'][1]\n",
    "ny = dims['Y'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Estimator\n",
    "\n",
    "To generate initial states during $N$-step training, we make use of state estimators which use the past outputs of the system to estimate the next state variable. NeuroMANCER provides a host of parametric state estimators for this purpose; in this case, we'll use a fully connected neural network. Each state estimator in neuromancer.estimators represents more generic component class representing arbitrary nn.Module with input_keys and output_keys for mapping neuromancer variabels.  \n",
    "For more details see components and constraints tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yp']\n",
      "['x0_estimator', 'reg_error_estimator']\n",
      "estimator(Yp) -> x0_estimator, reg_error_estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tuor369/anaconda3/envs/neuromancer/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "activation = activations['relu']\n",
    "linmap = slim.maps['linear']\n",
    "\n",
    "nx = 90  # size of the latent variables\n",
    "estimator = estimators.MLPEstimator(\n",
    "    {**dims, \"x0\": (nx,)},\n",
    "    nsteps=params['nsteps'],  # future window Nf\n",
    "    window_size=params['nsteps'],  # past window Np <= Nf\n",
    "    bias=True,\n",
    "    linear_map=linmap,\n",
    "    nonlin=activation,\n",
    "    hsizes=[90, 120, 90],\n",
    "    input_keys=[\"Yp\"],\n",
    "    linargs={},\n",
    "    name='estimator',\n",
    ")\n",
    "\n",
    "# input output keys of the estimator \n",
    "print(estimator.input_keys)\n",
    "print(estimator.output_keys)\n",
    "\n",
    "# dynamics_model mapping of input keys to output keys\n",
    "print(estimator)\n",
    "# x0 = estimator(Yp)\n",
    "# x0: initial values of latent variables estimated from time lagged outputs Yp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Space Model\n",
    "\n",
    "Next, we define our block-structured state space model (SSM) as neuromancer component. These models divide system dynamics into separate functional components to decouple the behaviors of inputs, state transitions, and outputs. Using the `BlockSSM` constructor, we can build SSMs with arbitrary components, each with their own nonlinearities and structured weight parameterizations.  \n",
    "For more details on how to construct arbitrary acyclic computational graphs with see components tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Uf', 'Yf', 'x0_estimator']\n",
      "['fU_dynamics', 'X_pred_dynamics', 'Y_pred_dynamics', 'reg_error_dynamics']\n",
      "dynamics(Uf, Yf, x0_estimator) -> fU_dynamics, X_pred_dynamics, Y_pred_dynamics, reg_error_dynamics\n"
     ]
    }
   ],
   "source": [
    "# define blocks\n",
    "fx = blocks.RNN(nx, nx, linear_map=linmap,\n",
    "                nonlin=activations['softexp'], hsizes=[60, 60])\n",
    "linargs = {\"sigma_min\": 0.5, \"sigma_max\": 1.0}\n",
    "fy = slim.maps['softSVD'](nx, ny, linargs=linargs)\n",
    "fu = blocks.MLP(nu, nx, hsizes=[60, 60]) if nu != 0 else None\n",
    "\n",
    "# define state space model\n",
    "dynamics_model = dynamics.BlockSSM(fx, fy, fu=fu, name='dynamics', xou=torch.add,\n",
    "                                   input_key_map={\"x0\": f\"x0_{estimator.name}\"})\n",
    "\n",
    "# input output keys of the SSM \n",
    "print(dynamics_model.input_keys)\n",
    "print(dynamics_model.output_keys)\n",
    "\n",
    "# dynamics_model mapping of input keys to output keys\n",
    "print(dynamics_model)\n",
    "# Yf = dynamics_model(x0, Uf)\n",
    "# Uf: future control actions\n",
    "# Yf: predicted outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuromancer Variables\n",
    "Here we leverage Neuromancer's variable abstraction for intuitive high-level definition of objective temrs and constraints. Each variable has assigned uniqye name that either exists as a key in the dataset dictionary (e.g., \"Yf\"), or is listed as output key in one of our components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounds\n",
    "xmin = -0.2\n",
    "xmax = 1.2\n",
    "dxudmin = -0.5\n",
    "dxudmax = 0.5\n",
    "\n",
    "# loss terms and constraints definition via variable class: neuromancer variable declaration\n",
    "yhat = Variable(f\"Y_pred_{dynamics_model.name}\")\n",
    "y = Variable(\"Yf\")\n",
    "x0 = Variable(f\"x0_{estimator.name}\")\n",
    "xhat = Variable(f\"X_pred_{dynamics_model.name}\")\n",
    "est_reg = Variable(f\"reg_error_{estimator.name}\")\n",
    "dyn_reg = Variable(f\"reg_error_{dynamics_model.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Terms\n",
    "To optimize the weights of our model, we first use mean-squared error as our objective to minimize the error between ground-truth observables and those predicted by our SSM. We additionally impose a loss on the estimator that ensures its state predictions align with ground-truth state observations. Further we add state smoothening penalty, and regularizations terms imposed on the model weights.  \n",
    "\n",
    "In addition to minimizing the mean-squared error of predicted and expected observables, we may also want to impose further constraints on different model components to enforce certain model behaviors. Our first constraint will ensure the smoothness of state transitions by minimizing the mean-squared error between consecutive state predictions. Our second and third constraints impose lower and upper bounds on output observations, specifically the interval $[-0.2, 1.2]$.\n",
    "\n",
    "For more details see constraints tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function terms\n",
    "reference_loss = params['Q_y']*((yhat == y)^2)\n",
    "estimator_loss = params['Q_e']*((x0[1:] == xhat[-1, :-1, :])^2)\n",
    "state_smoothing = params['Q_dx']*((xhat[1:] == xhat[:-1])^2)\n",
    "regularization = params['Q_sub']*((est_reg + dyn_reg == 0)^2)\n",
    "# define constraints\n",
    "observation_lower_bound_penalty = params['Q_con_x']*(yhat > xmin)\n",
    "observation_upper_bound_penalty = params['Q_con_x']*(yhat < xmax)\n",
    "# custom loss and constraints names used as metrics\n",
    "reference_loss.name = \"ref_loss\"\n",
    "estimator_loss.name = \"arrival_cost\"\n",
    "regularization.name = \"reg_error\"\n",
    "observation_lower_bound_penalty.name = \"y_low_bound_error\"\n",
    "observation_upper_bound_penalty.name = \"y_up_bound_error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "\n",
    "Having defined our model components, objective terms, and constraints, we can now combine each into an optimization problem using NeuroMANCER's `Problem` class. This will produce our final model, connecting all of the underlying components necessary to process our data, generate predictions, and optimize the model components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of objectives and constraints\n",
    "objectives = [regularization, reference_loss, estimator_loss]\n",
    "constraints = [\n",
    "    state_smoothing,\n",
    "    observation_lower_bound_penalty,\n",
    "    observation_upper_bound_penalty,\n",
    "]\n",
    "# list of components\n",
    "components = [estimator, dynamics_model]\n",
    "# constrained optimization problem = objectives + constraints + components\n",
    "model = Problem(objectives, constraints, components)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Our Model\n",
    "\n",
    "For training models, NeuroMANCER provides a `Trainer` class which encapsulates all of the training and evaluation logic necessary for $N$-step training, and additionally provides options for experiment logging, visualization, and roll-out simulation in the loop. To get started, we instantiate PyTorch's AdamW optimizer with our model's parameters. We then provide some components which are specific to the NeuroMANCER training regime:\n",
    "\n",
    "* The `OpenLoopSimulator` is used to simulate a full roll-out of the SSM starting from a single initial state. Unlike the $N$-step simulation used during training, this is meant to evaluate the performance of the model in a free-running manner rather than over short windows of time.\n",
    "* The `BasicLogger` is used to output user-specified metrics during training and evaluation. It also handles saving model checkpoints.\n",
    "* The `VisualizerOpen` class generates plots showing predicted vs. ground-truth outputs for both $N$-step and open-loop model simulations, providing a useful visual assessment of model performance.\n",
    "\n",
    "With all of these components defined, we can then instantiate our `Trainer` class. We provide the model and dataset, the optimizer, and the aforementioned helper components. We additionally specify the number of epochs, an evaluation metric used to determine the best-performing model, and a patience value; this latter value determines how many epochs should be completed without any improvement in the evaluation metric before discontinuing training in order to avoid overfitting and wasted computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Number of parameters: 85022\n"
     ]
    }
   ],
   "source": [
    "# define callback\n",
    "simulator = OpenLoopSimulator(\n",
    "    model, train_loop, dev_loop, test_loop, eval_sim=True, device=device,\n",
    ") if isinstance(train_loop, dict) else MultiSequenceOpenLoopSimulator(\n",
    "    model, train_loop, dev_loop, test_loop, eval_sim=True, device=device,\n",
    ")\n",
    "visualizer = VisualizerOpen(\n",
    "    dynamics_model,\n",
    "    1,\n",
    "    params['savedir'],\n",
    "    training_visuals=False,\n",
    "    trace_movie=False,\n",
    ")\n",
    "callback = SysIDCallback(simulator, visualizer)\n",
    "# define logger\n",
    "logger = BasicLogger(args=None, savedir= params['savedir'], verbosity=1, stdout=params['metrics'])\n",
    "# select optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=params['lr'])\n",
    "\n",
    "# define trainer\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    train_data,\n",
    "    dev_data,\n",
    "    test_data,\n",
    "    optimizer,\n",
    "    callback=callback,\n",
    "    logger=logger,\n",
    "    epochs=params['epochs'],\n",
    "    eval_metric=params['eval_metric'],\n",
    "    patience=params['patience'],\n",
    "    warmup=params['warmup'],\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we enter the training loop by calling the `train()` method on the `trainer` object; upon training completion, this method will return the best-observed model according to the evaluation metric specified previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tuor369/anaconda3/envs/neuromancer/lib/python3.8/site-packages/torch/nn/modules/container.py:435: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\tnstep_dev_ref_loss: 0.31286\tnstep_dev_loss: 0.45510\tloop_dev_ref_loss: 0.31246\tloop_dev_loss: nan\teltime:  4.10042\n",
      "epoch: 1\tnstep_dev_ref_loss: 0.28439\tnstep_dev_loss: 0.33467\tloop_dev_ref_loss: 0.28394\tloop_dev_loss: nan\teltime:  4.26508\n",
      "epoch: 2\tnstep_dev_ref_loss: 0.25881\tnstep_dev_loss: 0.32463\tloop_dev_ref_loss: 0.25832\tloop_dev_loss: nan\teltime:  4.41580\n",
      "epoch: 3\tnstep_dev_ref_loss: 0.23479\tnstep_dev_loss: 0.33127\tloop_dev_ref_loss: 0.23428\tloop_dev_loss: nan\teltime:  4.57513\n",
      "epoch: 4\tnstep_dev_ref_loss: 0.21251\tnstep_dev_loss: 0.30156\tloop_dev_ref_loss: 0.21199\tloop_dev_loss: nan\teltime:  4.72994\n",
      "epoch: 5\tnstep_dev_ref_loss: 0.19302\tnstep_dev_loss: 0.26531\tloop_dev_ref_loss: 0.19253\tloop_dev_loss: nan\teltime:  4.87832\n",
      "epoch: 6\tnstep_dev_ref_loss: 0.17780\tnstep_dev_loss: 0.24905\tloop_dev_ref_loss: 0.17738\tloop_dev_loss: nan\teltime:  5.02881\n",
      "epoch: 7\tnstep_dev_ref_loss: 0.16820\tnstep_dev_loss: 0.25671\tloop_dev_ref_loss: 0.16793\tloop_dev_loss: nan\teltime:  5.16455\n",
      "epoch: 8\tnstep_dev_ref_loss: 0.16412\tnstep_dev_loss: 0.26867\tloop_dev_ref_loss: 0.16403\tloop_dev_loss: nan\teltime:  5.34172\n",
      "epoch: 9\tnstep_dev_ref_loss: 0.16292\tnstep_dev_loss: 0.26613\tloop_dev_ref_loss: 0.16294\tloop_dev_loss: nan\teltime:  5.48164\n",
      "epoch: 10\tnstep_dev_ref_loss: 0.16153\tnstep_dev_loss: 0.24822\tloop_dev_ref_loss: 0.16153\tloop_dev_loss: nan\teltime:  5.62037\n",
      "epoch: 11\tnstep_dev_ref_loss: 0.15883\tnstep_dev_loss: 0.22656\tloop_dev_ref_loss: 0.15873\tloop_dev_loss: nan\teltime:  5.76645\n",
      "epoch: 12\tnstep_dev_ref_loss: 0.15524\tnstep_dev_loss: 0.21085\tloop_dev_ref_loss: 0.15503\tloop_dev_loss: nan\teltime:  5.90680\n",
      "epoch: 13\tnstep_dev_ref_loss: 0.15166\tnstep_dev_loss: 0.20283\tloop_dev_ref_loss: 0.15137\tloop_dev_loss: nan\teltime:  6.03756\n",
      "epoch: 14\tnstep_dev_ref_loss: 0.14885\tnstep_dev_loss: 0.19830\tloop_dev_ref_loss: 0.14851\tloop_dev_loss: nan\teltime:  6.17589\n",
      "epoch: 15\tnstep_dev_ref_loss: 0.14749\tnstep_dev_loss: 0.19386\tloop_dev_ref_loss: 0.14715\tloop_dev_loss: nan\teltime:  6.33329\n",
      "epoch: 16\tnstep_dev_ref_loss: 0.14816\tnstep_dev_loss: 0.19085\tloop_dev_ref_loss: 0.14785\tloop_dev_loss: nan\teltime:  6.46700\n",
      "epoch: 17\tnstep_dev_ref_loss: 0.15072\tnstep_dev_loss: 0.19179\tloop_dev_ref_loss: 0.15048\tloop_dev_loss: nan\teltime:  6.60875\n",
      "epoch: 18\tnstep_dev_ref_loss: 0.15366\tnstep_dev_loss: 0.19535\tloop_dev_ref_loss: 0.15347\tloop_dev_loss: nan\teltime:  6.74919\n",
      "epoch: 19\tnstep_dev_ref_loss: 0.15482\tnstep_dev_loss: 0.19723\tloop_dev_ref_loss: 0.15466\tloop_dev_loss: nan\teltime:  6.89333\n",
      "epoch: 20\tnstep_dev_ref_loss: 0.15351\tnstep_dev_loss: 0.19517\tloop_dev_ref_loss: 0.15330\tloop_dev_loss: nan\teltime:  7.02806\n",
      "epoch: 21\tnstep_dev_ref_loss: 0.15044\tnstep_dev_loss: 0.19048\tloop_dev_ref_loss: 0.15016\tloop_dev_loss: nan\teltime:  7.16331\n",
      "epoch: 22\tnstep_dev_ref_loss: 0.14677\tnstep_dev_loss: 0.18611\tloop_dev_ref_loss: 0.14641\tloop_dev_loss: nan\teltime:  7.30951\n",
      "epoch: 23\tnstep_dev_ref_loss: 0.14374\tnstep_dev_loss: 0.18504\tloop_dev_ref_loss: 0.14331\tloop_dev_loss: nan\teltime:  7.45109\n",
      "epoch: 24\tnstep_dev_ref_loss: 0.14235\tnstep_dev_loss: 0.18874\tloop_dev_ref_loss: 0.14188\tloop_dev_loss: nan\teltime:  7.58807\n",
      "epoch: 25\tnstep_dev_ref_loss: 0.14241\tnstep_dev_loss: 0.19493\tloop_dev_ref_loss: 0.14190\tloop_dev_loss: nan\teltime:  7.73980\n",
      "epoch: 26\tnstep_dev_ref_loss: 0.14232\tnstep_dev_loss: 0.19862\tloop_dev_ref_loss: 0.14174\tloop_dev_loss: nan\teltime:  7.89523\n",
      "epoch: 27\tnstep_dev_ref_loss: 0.14091\tnstep_dev_loss: 0.19811\tloop_dev_ref_loss: 0.14019\tloop_dev_loss: nan\teltime:  8.03502\n",
      "epoch: 28\tnstep_dev_ref_loss: 0.13756\tnstep_dev_loss: 0.19596\tloop_dev_ref_loss: 0.13670\tloop_dev_loss: nan\teltime:  8.17612\n",
      "epoch: 29\tnstep_dev_ref_loss: 0.13277\tnstep_dev_loss: 0.19518\tloop_dev_ref_loss: 0.13188\tloop_dev_loss: nan\teltime:  8.31215\n",
      "epoch: 30\tnstep_dev_ref_loss: 0.12822\tnstep_dev_loss: 0.19486\tloop_dev_ref_loss: 0.12737\tloop_dev_loss: nan\teltime:  8.45182\n",
      "epoch: 31\tnstep_dev_ref_loss: 0.12376\tnstep_dev_loss: 0.18876\tloop_dev_ref_loss: 0.12281\tloop_dev_loss: nan\teltime:  8.59382\n",
      "epoch: 32\tnstep_dev_ref_loss: 0.12016\tnstep_dev_loss: 0.17914\tloop_dev_ref_loss: 0.11903\tloop_dev_loss: nan\teltime:  8.75326\n",
      "epoch: 33\tnstep_dev_ref_loss: 0.11829\tnstep_dev_loss: 0.17221\tloop_dev_ref_loss: 0.11706\tloop_dev_loss: nan\teltime:  8.88555\n",
      "epoch: 34\tnstep_dev_ref_loss: 0.11715\tnstep_dev_loss: 0.16882\tloop_dev_ref_loss: 0.11592\tloop_dev_loss: nan\teltime:  9.03492\n",
      "epoch: 35\tnstep_dev_ref_loss: 0.11613\tnstep_dev_loss: 0.16710\tloop_dev_ref_loss: 0.11492\tloop_dev_loss: nan\teltime:  9.18255\n",
      "epoch: 36\tnstep_dev_ref_loss: 0.11523\tnstep_dev_loss: 0.16521\tloop_dev_ref_loss: 0.11404\tloop_dev_loss: nan\teltime:  9.34494\n",
      "epoch: 37\tnstep_dev_ref_loss: 0.11444\tnstep_dev_loss: 0.16241\tloop_dev_ref_loss: 0.11327\tloop_dev_loss: nan\teltime:  9.49317\n",
      "epoch: 38\tnstep_dev_ref_loss: 0.11380\tnstep_dev_loss: 0.15915\tloop_dev_ref_loss: 0.11265\tloop_dev_loss: nan\teltime:  9.63102\n",
      "epoch: 39\tnstep_dev_ref_loss: 0.11317\tnstep_dev_loss: 0.15617\tloop_dev_ref_loss: 0.11205\tloop_dev_loss: nan\teltime:  9.80082\n",
      "epoch: 40\tnstep_dev_ref_loss: 0.11232\tnstep_dev_loss: 0.15386\tloop_dev_ref_loss: 0.11122\tloop_dev_loss: nan\teltime:  9.94694\n",
      "epoch: 41\tnstep_dev_ref_loss: 0.11125\tnstep_dev_loss: 0.15246\tloop_dev_ref_loss: 0.11018\tloop_dev_loss: nan\teltime:  10.08761\n",
      "epoch: 42\tnstep_dev_ref_loss: 0.11026\tnstep_dev_loss: 0.15208\tloop_dev_ref_loss: 0.10920\tloop_dev_loss: nan\teltime:  10.22773\n",
      "epoch: 43\tnstep_dev_ref_loss: 0.10955\tnstep_dev_loss: 0.15251\tloop_dev_ref_loss: 0.10849\tloop_dev_loss: nan\teltime:  10.39328\n",
      "epoch: 44\tnstep_dev_ref_loss: 0.10902\tnstep_dev_loss: 0.15314\tloop_dev_ref_loss: 0.10797\tloop_dev_loss: nan\teltime:  10.53256\n",
      "epoch: 45\tnstep_dev_ref_loss: 0.10836\tnstep_dev_loss: 0.15322\tloop_dev_ref_loss: 0.10732\tloop_dev_loss: nan\teltime:  10.68070\n",
      "epoch: 46\tnstep_dev_ref_loss: 0.10728\tnstep_dev_loss: 0.15232\tloop_dev_ref_loss: 0.10624\tloop_dev_loss: nan\teltime:  10.84566\n",
      "epoch: 47\tnstep_dev_ref_loss: 0.10574\tnstep_dev_loss: 0.15059\tloop_dev_ref_loss: 0.10470\tloop_dev_loss: nan\teltime:  10.97791\n",
      "epoch: 48\tnstep_dev_ref_loss: 0.10406\tnstep_dev_loss: 0.14868\tloop_dev_ref_loss: 0.10302\tloop_dev_loss: nan\teltime:  11.12013\n",
      "epoch: 49\tnstep_dev_ref_loss: 0.10266\tnstep_dev_loss: 0.14734\tloop_dev_ref_loss: 0.10163\tloop_dev_loss: nan\teltime:  11.26171\n",
      "epoch: 50\tnstep_dev_ref_loss: 0.10188\tnstep_dev_loss: 0.14695\tloop_dev_ref_loss: 0.10085\tloop_dev_loss: nan\teltime:  11.40682\n",
      "epoch: 51\tnstep_dev_ref_loss: 0.10172\tnstep_dev_loss: 0.14736\tloop_dev_ref_loss: 0.10070\tloop_dev_loss: nan\teltime:  11.54486\n",
      "epoch: 52\tnstep_dev_ref_loss: 0.10189\tnstep_dev_loss: 0.14803\tloop_dev_ref_loss: 0.10089\tloop_dev_loss: nan\teltime:  11.69454\n",
      "epoch: 53\tnstep_dev_ref_loss: 0.10200\tnstep_dev_loss: 0.14834\tloop_dev_ref_loss: 0.10101\tloop_dev_loss: nan\teltime:  11.82237\n",
      "epoch: 54\tnstep_dev_ref_loss: 0.10179\tnstep_dev_loss: 0.14795\tloop_dev_ref_loss: 0.10081\tloop_dev_loss: nan\teltime:  11.97600\n",
      "epoch: 55\tnstep_dev_ref_loss: 0.10129\tnstep_dev_loss: 0.14694\tloop_dev_ref_loss: 0.10032\tloop_dev_loss: nan\teltime:  12.11760\n",
      "epoch: 56\tnstep_dev_ref_loss: 0.10072\tnstep_dev_loss: 0.14567\tloop_dev_ref_loss: 0.09977\tloop_dev_loss: nan\teltime:  12.26372\n",
      "epoch: 57\tnstep_dev_ref_loss: 0.10033\tnstep_dev_loss: 0.14453\tloop_dev_ref_loss: 0.09940\tloop_dev_loss: nan\teltime:  12.39782\n",
      "epoch: 58\tnstep_dev_ref_loss: 0.10022\tnstep_dev_loss: 0.14371\tloop_dev_ref_loss: 0.09931\tloop_dev_loss: nan\teltime:  12.54730\n",
      "epoch: 59\tnstep_dev_ref_loss: 0.10031\tnstep_dev_loss: 0.14317\tloop_dev_ref_loss: 0.09942\tloop_dev_loss: nan\teltime:  12.71215\n",
      "epoch: 60\tnstep_dev_ref_loss: 0.10040\tnstep_dev_loss: 0.14271\tloop_dev_ref_loss: 0.09953\tloop_dev_loss: nan\teltime:  12.90826\n",
      "epoch: 61\tnstep_dev_ref_loss: 0.10030\tnstep_dev_loss: 0.14204\tloop_dev_ref_loss: 0.09946\tloop_dev_loss: nan\teltime:  13.08023\n",
      "epoch: 62\tnstep_dev_ref_loss: 0.09999\tnstep_dev_loss: 0.14110\tloop_dev_ref_loss: 0.09917\tloop_dev_loss: nan\teltime:  13.23199\n",
      "epoch: 63\tnstep_dev_ref_loss: 0.09957\tnstep_dev_loss: 0.13996\tloop_dev_ref_loss: 0.09876\tloop_dev_loss: nan\teltime:  13.36510\n",
      "epoch: 64\tnstep_dev_ref_loss: 0.09918\tnstep_dev_loss: 0.13884\tloop_dev_ref_loss: 0.09839\tloop_dev_loss: nan\teltime:  13.52244\n",
      "epoch: 65\tnstep_dev_ref_loss: 0.09892\tnstep_dev_loss: 0.13795\tloop_dev_ref_loss: 0.09814\tloop_dev_loss: nan\teltime:  13.67241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 66\tnstep_dev_ref_loss: 0.09878\tnstep_dev_loss: 0.13736\tloop_dev_ref_loss: 0.09801\tloop_dev_loss: nan\teltime:  13.82958\n",
      "epoch: 67\tnstep_dev_ref_loss: 0.09866\tnstep_dev_loss: 0.13701\tloop_dev_ref_loss: 0.09790\tloop_dev_loss: nan\teltime:  13.98176\n",
      "epoch: 68\tnstep_dev_ref_loss: 0.09843\tnstep_dev_loss: 0.13671\tloop_dev_ref_loss: 0.09769\tloop_dev_loss: nan\teltime:  14.14168\n",
      "epoch: 69\tnstep_dev_ref_loss: 0.09803\tnstep_dev_loss: 0.13628\tloop_dev_ref_loss: 0.09729\tloop_dev_loss: nan\teltime:  14.32669\n",
      "epoch: 70\tnstep_dev_ref_loss: 0.09746\tnstep_dev_loss: 0.13566\tloop_dev_ref_loss: 0.09673\tloop_dev_loss: nan\teltime:  14.48147\n",
      "epoch: 71\tnstep_dev_ref_loss: 0.09680\tnstep_dev_loss: 0.13489\tloop_dev_ref_loss: 0.09606\tloop_dev_loss: nan\teltime:  14.62964\n",
      "epoch: 72\tnstep_dev_ref_loss: 0.09612\tnstep_dev_loss: 0.13411\tloop_dev_ref_loss: 0.09538\tloop_dev_loss: nan\teltime:  14.77485\n",
      "epoch: 73\tnstep_dev_ref_loss: 0.09547\tnstep_dev_loss: 0.13345\tloop_dev_ref_loss: 0.09474\tloop_dev_loss: nan\teltime:  14.91593\n",
      "epoch: 74\tnstep_dev_ref_loss: 0.09487\tnstep_dev_loss: 0.13296\tloop_dev_ref_loss: 0.09414\tloop_dev_loss: nan\teltime:  15.05998\n",
      "epoch: 75\tnstep_dev_ref_loss: 0.09424\tnstep_dev_loss: 0.13252\tloop_dev_ref_loss: 0.09351\tloop_dev_loss: nan\teltime:  15.20112\n",
      "epoch: 76\tnstep_dev_ref_loss: 0.09353\tnstep_dev_loss: 0.13199\tloop_dev_ref_loss: 0.09280\tloop_dev_loss: nan\teltime:  15.34913\n",
      "epoch: 77\tnstep_dev_ref_loss: 0.09271\tnstep_dev_loss: 0.13123\tloop_dev_ref_loss: 0.09199\tloop_dev_loss: nan\teltime:  15.49451\n",
      "epoch: 78\tnstep_dev_ref_loss: 0.09184\tnstep_dev_loss: 0.13031\tloop_dev_ref_loss: 0.09112\tloop_dev_loss: nan\teltime:  15.64962\n",
      "epoch: 79\tnstep_dev_ref_loss: 0.09098\tnstep_dev_loss: 0.12934\tloop_dev_ref_loss: 0.09027\tloop_dev_loss: nan\teltime:  15.79421\n",
      "epoch: 80\tnstep_dev_ref_loss: 0.09020\tnstep_dev_loss: 0.12845\tloop_dev_ref_loss: 0.08948\tloop_dev_loss: nan\teltime:  15.94832\n",
      "epoch: 81\tnstep_dev_ref_loss: 0.08949\tnstep_dev_loss: 0.12766\tloop_dev_ref_loss: 0.08877\tloop_dev_loss: nan\teltime:  16.09059\n",
      "epoch: 82\tnstep_dev_ref_loss: 0.08879\tnstep_dev_loss: 0.12689\tloop_dev_ref_loss: 0.08808\tloop_dev_loss: nan\teltime:  16.24750\n",
      "epoch: 83\tnstep_dev_ref_loss: 0.08807\tnstep_dev_loss: 0.12602\tloop_dev_ref_loss: 0.08736\tloop_dev_loss: nan\teltime:  16.39639\n",
      "epoch: 84\tnstep_dev_ref_loss: 0.08728\tnstep_dev_loss: 0.12504\tloop_dev_ref_loss: 0.08658\tloop_dev_loss: nan\teltime:  16.55075\n",
      "epoch: 85\tnstep_dev_ref_loss: 0.08647\tnstep_dev_loss: 0.12397\tloop_dev_ref_loss: 0.08578\tloop_dev_loss: nan\teltime:  16.70821\n",
      "epoch: 86\tnstep_dev_ref_loss: 0.08569\tnstep_dev_loss: 0.12297\tloop_dev_ref_loss: 0.08500\tloop_dev_loss: nan\teltime:  16.85967\n",
      "epoch: 87\tnstep_dev_ref_loss: 0.08498\tnstep_dev_loss: 0.12211\tloop_dev_ref_loss: 0.08430\tloop_dev_loss: nan\teltime:  17.01076\n",
      "epoch: 88\tnstep_dev_ref_loss: 0.08433\tnstep_dev_loss: 0.12139\tloop_dev_ref_loss: 0.08367\tloop_dev_loss: nan\teltime:  17.14846\n",
      "epoch: 89\tnstep_dev_ref_loss: 0.08371\tnstep_dev_loss: 0.12072\tloop_dev_ref_loss: 0.08305\tloop_dev_loss: nan\teltime:  17.29758\n",
      "epoch: 90\tnstep_dev_ref_loss: 0.08307\tnstep_dev_loss: 0.12004\tloop_dev_ref_loss: 0.08242\tloop_dev_loss: nan\teltime:  17.44407\n",
      "epoch: 91\tnstep_dev_ref_loss: 0.08243\tnstep_dev_loss: 0.11933\tloop_dev_ref_loss: 0.08178\tloop_dev_loss: nan\teltime:  17.59606\n",
      "epoch: 92\tnstep_dev_ref_loss: 0.08182\tnstep_dev_loss: 0.11868\tloop_dev_ref_loss: 0.08119\tloop_dev_loss: nan\teltime:  17.77392\n",
      "epoch: 93\tnstep_dev_ref_loss: 0.08128\tnstep_dev_loss: 0.11814\tloop_dev_ref_loss: 0.08065\tloop_dev_loss: nan\teltime:  17.96474\n",
      "epoch: 94\tnstep_dev_ref_loss: 0.08079\tnstep_dev_loss: 0.11775\tloop_dev_ref_loss: 0.08017\tloop_dev_loss: nan\teltime:  18.14450\n",
      "epoch: 95\tnstep_dev_ref_loss: 0.08032\tnstep_dev_loss: 0.11732\tloop_dev_ref_loss: 0.07971\tloop_dev_loss: nan\teltime:  18.31709\n",
      "epoch: 96\tnstep_dev_ref_loss: 0.07984\tnstep_dev_loss: 0.11698\tloop_dev_ref_loss: 0.07924\tloop_dev_loss: nan\teltime:  18.48719\n",
      "epoch: 97\tnstep_dev_ref_loss: 0.07936\tnstep_dev_loss: 0.11669\tloop_dev_ref_loss: 0.07877\tloop_dev_loss: nan\teltime:  18.62664\n",
      "epoch: 98\tnstep_dev_ref_loss: 0.07888\tnstep_dev_loss: 0.11637\tloop_dev_ref_loss: 0.07830\tloop_dev_loss: nan\teltime:  18.77081\n",
      "epoch: 99\tnstep_dev_ref_loss: 0.07841\tnstep_dev_loss: 0.11599\tloop_dev_ref_loss: 0.07785\tloop_dev_loss: nan\teltime:  18.91936\n",
      "epoch: 100\tnstep_dev_ref_loss: 0.07796\tnstep_dev_loss: 0.11559\tloop_dev_ref_loss: 0.07740\tloop_dev_loss: nan\teltime:  19.06383\n",
      "epoch: 101\tnstep_dev_ref_loss: 0.07753\tnstep_dev_loss: 0.11531\tloop_dev_ref_loss: 0.07699\tloop_dev_loss: nan\teltime:  19.20278\n",
      "epoch: 102\tnstep_dev_ref_loss: 0.07713\tnstep_dev_loss: 0.11519\tloop_dev_ref_loss: 0.07660\tloop_dev_loss: nan\teltime:  19.34192\n",
      "epoch: 103\tnstep_dev_ref_loss: 0.07675\tnstep_dev_loss: 0.11500\tloop_dev_ref_loss: 0.07624\tloop_dev_loss: nan\teltime:  19.48229\n",
      "epoch: 104\tnstep_dev_ref_loss: 0.07639\tnstep_dev_loss: 0.11479\tloop_dev_ref_loss: 0.07589\tloop_dev_loss: nan\teltime:  19.63403\n",
      "epoch: 105\tnstep_dev_ref_loss: 0.07603\tnstep_dev_loss: 0.11475\tloop_dev_ref_loss: 0.07555\tloop_dev_loss: nan\teltime:  19.77586\n",
      "epoch: 106\tnstep_dev_ref_loss: 0.07566\tnstep_dev_loss: 0.11472\tloop_dev_ref_loss: 0.07519\tloop_dev_loss: nan\teltime:  19.92908\n",
      "epoch: 107\tnstep_dev_ref_loss: 0.07525\tnstep_dev_loss: 0.11462\tloop_dev_ref_loss: 0.07480\tloop_dev_loss: nan\teltime:  20.06668\n",
      "epoch: 108\tnstep_dev_ref_loss: 0.07485\tnstep_dev_loss: 0.11448\tloop_dev_ref_loss: 0.07442\tloop_dev_loss: nan\teltime:  20.21179\n",
      "epoch: 109\tnstep_dev_ref_loss: 0.07451\tnstep_dev_loss: 0.11452\tloop_dev_ref_loss: 0.07410\tloop_dev_loss: nan\teltime:  20.35874\n",
      "epoch: 110\tnstep_dev_ref_loss: 0.07423\tnstep_dev_loss: 0.11483\tloop_dev_ref_loss: 0.07385\tloop_dev_loss: nan\teltime:  20.50866\n",
      "epoch: 111\tnstep_dev_ref_loss: 0.07398\tnstep_dev_loss: 0.11520\tloop_dev_ref_loss: 0.07361\tloop_dev_loss: nan\teltime:  20.65177\n",
      "epoch: 112\tnstep_dev_ref_loss: 0.07371\tnstep_dev_loss: 0.11551\tloop_dev_ref_loss: 0.07337\tloop_dev_loss: nan\teltime:  20.80923\n",
      "epoch: 113\tnstep_dev_ref_loss: 0.07345\tnstep_dev_loss: 0.11593\tloop_dev_ref_loss: 0.07314\tloop_dev_loss: nan\teltime:  20.97695\n",
      "epoch: 114\tnstep_dev_ref_loss: 0.07323\tnstep_dev_loss: 0.11650\tloop_dev_ref_loss: 0.07295\tloop_dev_loss: nan\teltime:  21.11951\n",
      "epoch: 115\tnstep_dev_ref_loss: 0.07303\tnstep_dev_loss: 0.11713\tloop_dev_ref_loss: 0.07278\tloop_dev_loss: nan\teltime:  21.26090\n",
      "epoch: 116\tnstep_dev_ref_loss: 0.07280\tnstep_dev_loss: 0.11760\tloop_dev_ref_loss: 0.07258\tloop_dev_loss: nan\teltime:  21.40826\n",
      "epoch: 117\tnstep_dev_ref_loss: 0.07254\tnstep_dev_loss: 0.11800\tloop_dev_ref_loss: 0.07235\tloop_dev_loss: nan\teltime:  21.57011\n",
      "epoch: 118\tnstep_dev_ref_loss: 0.07228\tnstep_dev_loss: 0.11843\tloop_dev_ref_loss: 0.07212\tloop_dev_loss: nan\teltime:  21.72050\n",
      "epoch: 119\tnstep_dev_ref_loss: 0.07203\tnstep_dev_loss: 0.11889\tloop_dev_ref_loss: 0.07189\tloop_dev_loss: nan\teltime:  21.87605\n",
      "epoch: 120\tnstep_dev_ref_loss: 0.07176\tnstep_dev_loss: 0.11930\tloop_dev_ref_loss: 0.07164\tloop_dev_loss: nan\teltime:  22.03084\n",
      "epoch: 121\tnstep_dev_ref_loss: 0.07148\tnstep_dev_loss: 0.11967\tloop_dev_ref_loss: 0.07138\tloop_dev_loss: nan\teltime:  22.17279\n",
      "epoch: 122\tnstep_dev_ref_loss: 0.07119\tnstep_dev_loss: 0.12000\tloop_dev_ref_loss: 0.07110\tloop_dev_loss: nan\teltime:  22.32084\n",
      "epoch: 123\tnstep_dev_ref_loss: 0.07089\tnstep_dev_loss: 0.12036\tloop_dev_ref_loss: 0.07083\tloop_dev_loss: nan\teltime:  22.46717\n",
      "epoch: 124\tnstep_dev_ref_loss: 0.07060\tnstep_dev_loss: 0.12071\tloop_dev_ref_loss: 0.07055\tloop_dev_loss: nan\teltime:  22.63549\n",
      "epoch: 125\tnstep_dev_ref_loss: 0.07027\tnstep_dev_loss: 0.12096\tloop_dev_ref_loss: 0.07023\tloop_dev_loss: nan\teltime:  22.78429\n",
      "epoch: 126\tnstep_dev_ref_loss: 0.06991\tnstep_dev_loss: 0.12110\tloop_dev_ref_loss: 0.06988\tloop_dev_loss: nan\teltime:  22.93400\n",
      "epoch: 127\tnstep_dev_ref_loss: 0.06955\tnstep_dev_loss: 0.12112\tloop_dev_ref_loss: 0.06951\tloop_dev_loss: nan\teltime:  23.06746\n",
      "epoch: 128\tnstep_dev_ref_loss: 0.06916\tnstep_dev_loss: 0.12099\tloop_dev_ref_loss: 0.06913\tloop_dev_loss: nan\teltime:  23.23624\n",
      "epoch: 129\tnstep_dev_ref_loss: 0.06876\tnstep_dev_loss: 0.12071\tloop_dev_ref_loss: 0.06873\tloop_dev_loss: nan\teltime:  23.37558\n",
      "epoch: 130\tnstep_dev_ref_loss: 0.06834\tnstep_dev_loss: 0.12027\tloop_dev_ref_loss: 0.06830\tloop_dev_loss: nan\teltime:  23.52099\n",
      "epoch: 131\tnstep_dev_ref_loss: 0.06791\tnstep_dev_loss: 0.11974\tloop_dev_ref_loss: 0.06785\tloop_dev_loss: nan\teltime:  23.67488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 132\tnstep_dev_ref_loss: 0.06751\tnstep_dev_loss: 0.11924\tloop_dev_ref_loss: 0.06743\tloop_dev_loss: nan\teltime:  23.82468\n",
      "epoch: 133\tnstep_dev_ref_loss: 0.06711\tnstep_dev_loss: 0.11877\tloop_dev_ref_loss: 0.06702\tloop_dev_loss: nan\teltime:  23.97076\n",
      "epoch: 134\tnstep_dev_ref_loss: 0.06668\tnstep_dev_loss: 0.11819\tloop_dev_ref_loss: 0.06658\tloop_dev_loss: nan\teltime:  24.11173\n",
      "epoch: 135\tnstep_dev_ref_loss: 0.06627\tnstep_dev_loss: 0.11761\tloop_dev_ref_loss: 0.06616\tloop_dev_loss: nan\teltime:  24.27135\n",
      "epoch: 136\tnstep_dev_ref_loss: 0.06590\tnstep_dev_loss: 0.11715\tloop_dev_ref_loss: 0.06578\tloop_dev_loss: nan\teltime:  24.42984\n",
      "epoch: 137\tnstep_dev_ref_loss: 0.06557\tnstep_dev_loss: 0.11676\tloop_dev_ref_loss: 0.06544\tloop_dev_loss: nan\teltime:  24.58096\n",
      "epoch: 138\tnstep_dev_ref_loss: 0.06524\tnstep_dev_loss: 0.11637\tloop_dev_ref_loss: 0.06510\tloop_dev_loss: nan\teltime:  24.74063\n",
      "epoch: 139\tnstep_dev_ref_loss: 0.06489\tnstep_dev_loss: 0.11591\tloop_dev_ref_loss: 0.06474\tloop_dev_loss: nan\teltime:  24.88594\n",
      "epoch: 140\tnstep_dev_ref_loss: 0.06452\tnstep_dev_loss: 0.11539\tloop_dev_ref_loss: 0.06436\tloop_dev_loss: nan\teltime:  25.02822\n",
      "epoch: 141\tnstep_dev_ref_loss: 0.06415\tnstep_dev_loss: 0.11486\tloop_dev_ref_loss: 0.06397\tloop_dev_loss: nan\teltime:  25.18916\n",
      "epoch: 142\tnstep_dev_ref_loss: 0.06375\tnstep_dev_loss: 0.11428\tloop_dev_ref_loss: 0.06356\tloop_dev_loss: nan\teltime:  25.33922\n",
      "epoch: 143\tnstep_dev_ref_loss: 0.06331\tnstep_dev_loss: 0.11362\tloop_dev_ref_loss: 0.06311\tloop_dev_loss: nan\teltime:  25.48444\n",
      "epoch: 144\tnstep_dev_ref_loss: 0.06286\tnstep_dev_loss: 0.11293\tloop_dev_ref_loss: 0.06264\tloop_dev_loss: nan\teltime:  25.62482\n",
      "epoch: 145\tnstep_dev_ref_loss: 0.06243\tnstep_dev_loss: 0.11231\tloop_dev_ref_loss: 0.06220\tloop_dev_loss: nan\teltime:  25.78471\n",
      "epoch: 146\tnstep_dev_ref_loss: 0.06200\tnstep_dev_loss: 0.11172\tloop_dev_ref_loss: 0.06176\tloop_dev_loss: nan\teltime:  25.92936\n",
      "epoch: 147\tnstep_dev_ref_loss: 0.06157\tnstep_dev_loss: 0.11114\tloop_dev_ref_loss: 0.06131\tloop_dev_loss: nan\teltime:  26.07291\n",
      "epoch: 148\tnstep_dev_ref_loss: 0.06112\tnstep_dev_loss: 0.11056\tloop_dev_ref_loss: 0.06086\tloop_dev_loss: nan\teltime:  26.21551\n",
      "epoch: 149\tnstep_dev_ref_loss: 0.06068\tnstep_dev_loss: 0.10999\tloop_dev_ref_loss: 0.06041\tloop_dev_loss: nan\teltime:  26.36159\n",
      "epoch: 150\tnstep_dev_ref_loss: 0.06026\tnstep_dev_loss: 0.10945\tloop_dev_ref_loss: 0.05998\tloop_dev_loss: nan\teltime:  26.50929\n",
      "epoch: 151\tnstep_dev_ref_loss: 0.05986\tnstep_dev_loss: 0.10891\tloop_dev_ref_loss: 0.05957\tloop_dev_loss: nan\teltime:  26.66547\n",
      "epoch: 152\tnstep_dev_ref_loss: 0.05947\tnstep_dev_loss: 0.10837\tloop_dev_ref_loss: 0.05916\tloop_dev_loss: nan\teltime:  26.81396\n",
      "epoch: 153\tnstep_dev_ref_loss: 0.05908\tnstep_dev_loss: 0.10784\tloop_dev_ref_loss: 0.05877\tloop_dev_loss: nan\teltime:  26.95586\n",
      "epoch: 154\tnstep_dev_ref_loss: 0.05871\tnstep_dev_loss: 0.10735\tloop_dev_ref_loss: 0.05839\tloop_dev_loss: nan\teltime:  27.12294\n",
      "epoch: 155\tnstep_dev_ref_loss: 0.05834\tnstep_dev_loss: 0.10685\tloop_dev_ref_loss: 0.05801\tloop_dev_loss: nan\teltime:  27.28405\n",
      "epoch: 156\tnstep_dev_ref_loss: 0.05798\tnstep_dev_loss: 0.10635\tloop_dev_ref_loss: 0.05764\tloop_dev_loss: nan\teltime:  27.45711\n",
      "epoch: 157\tnstep_dev_ref_loss: 0.05764\tnstep_dev_loss: 0.10589\tloop_dev_ref_loss: 0.05729\tloop_dev_loss: nan\teltime:  27.60549\n",
      "epoch: 158\tnstep_dev_ref_loss: 0.05733\tnstep_dev_loss: 0.10548\tloop_dev_ref_loss: 0.05697\tloop_dev_loss: nan\teltime:  27.75702\n",
      "epoch: 159\tnstep_dev_ref_loss: 0.05704\tnstep_dev_loss: 0.10511\tloop_dev_ref_loss: 0.05667\tloop_dev_loss: nan\teltime:  27.92103\n",
      "epoch: 160\tnstep_dev_ref_loss: 0.05676\tnstep_dev_loss: 0.10474\tloop_dev_ref_loss: 0.05639\tloop_dev_loss: nan\teltime:  28.07420\n",
      "epoch: 161\tnstep_dev_ref_loss: 0.05647\tnstep_dev_loss: 0.10437\tloop_dev_ref_loss: 0.05610\tloop_dev_loss: nan\teltime:  28.22522\n",
      "epoch: 162\tnstep_dev_ref_loss: 0.05620\tnstep_dev_loss: 0.10401\tloop_dev_ref_loss: 0.05581\tloop_dev_loss: nan\teltime:  28.37082\n",
      "epoch: 163\tnstep_dev_ref_loss: 0.05592\tnstep_dev_loss: 0.10366\tloop_dev_ref_loss: 0.05553\tloop_dev_loss: nan\teltime:  28.52822\n",
      "epoch: 164\tnstep_dev_ref_loss: 0.05563\tnstep_dev_loss: 0.10330\tloop_dev_ref_loss: 0.05524\tloop_dev_loss: nan\teltime:  28.67636\n",
      "epoch: 165\tnstep_dev_ref_loss: 0.05535\tnstep_dev_loss: 0.10295\tloop_dev_ref_loss: 0.05495\tloop_dev_loss: nan\teltime:  28.82347\n",
      "epoch: 166\tnstep_dev_ref_loss: 0.05508\tnstep_dev_loss: 0.10263\tloop_dev_ref_loss: 0.05467\tloop_dev_loss: nan\teltime:  28.97666\n",
      "epoch: 167\tnstep_dev_ref_loss: 0.05481\tnstep_dev_loss: 0.10231\tloop_dev_ref_loss: 0.05439\tloop_dev_loss: nan\teltime:  29.11500\n",
      "epoch: 168\tnstep_dev_ref_loss: 0.05454\tnstep_dev_loss: 0.10201\tloop_dev_ref_loss: 0.05412\tloop_dev_loss: nan\teltime:  29.26212\n",
      "epoch: 169\tnstep_dev_ref_loss: 0.05428\tnstep_dev_loss: 0.10173\tloop_dev_ref_loss: 0.05385\tloop_dev_loss: nan\teltime:  29.41177\n",
      "epoch: 170\tnstep_dev_ref_loss: 0.05402\tnstep_dev_loss: 0.10145\tloop_dev_ref_loss: 0.05358\tloop_dev_loss: nan\teltime:  29.56558\n",
      "epoch: 171\tnstep_dev_ref_loss: 0.05376\tnstep_dev_loss: 0.10120\tloop_dev_ref_loss: 0.05332\tloop_dev_loss: nan\teltime:  29.71028\n",
      "epoch: 172\tnstep_dev_ref_loss: 0.05350\tnstep_dev_loss: 0.10093\tloop_dev_ref_loss: 0.05305\tloop_dev_loss: nan\teltime:  29.86709\n",
      "epoch: 173\tnstep_dev_ref_loss: 0.05325\tnstep_dev_loss: 0.10073\tloop_dev_ref_loss: 0.05279\tloop_dev_loss: nan\teltime:  30.02534\n",
      "epoch: 174\tnstep_dev_ref_loss: 0.05298\tnstep_dev_loss: 0.10043\tloop_dev_ref_loss: 0.05252\tloop_dev_loss: nan\teltime:  30.16701\n",
      "epoch: 175\tnstep_dev_ref_loss: 0.05275\tnstep_dev_loss: 0.10029\tloop_dev_ref_loss: 0.05229\tloop_dev_loss: nan\teltime:  30.31088\n",
      "epoch: 176\tnstep_dev_ref_loss: 0.05250\tnstep_dev_loss: 0.10009\tloop_dev_ref_loss: 0.05203\tloop_dev_loss: nan\teltime:  30.45105\n",
      "epoch: 177\tnstep_dev_ref_loss: 0.05221\tnstep_dev_loss: 0.09982\tloop_dev_ref_loss: 0.05173\tloop_dev_loss: nan\teltime:  30.59008\n",
      "epoch: 178\tnstep_dev_ref_loss: 0.05191\tnstep_dev_loss: 0.09956\tloop_dev_ref_loss: 0.05142\tloop_dev_loss: nan\teltime:  30.74080\n",
      "epoch: 179\tnstep_dev_ref_loss: 0.05158\tnstep_dev_loss: 0.09908\tloop_dev_ref_loss: 0.05109\tloop_dev_loss: nan\teltime:  30.89830\n",
      "epoch: 180\tnstep_dev_ref_loss: 0.05141\tnstep_dev_loss: 0.09910\tloop_dev_ref_loss: 0.05091\tloop_dev_loss: nan\teltime:  31.03566\n",
      "epoch: 181\tnstep_dev_ref_loss: 0.05117\tnstep_dev_loss: 0.09890\tloop_dev_ref_loss: 0.05067\tloop_dev_loss: nan\teltime:  31.19852\n",
      "epoch: 182\tnstep_dev_ref_loss: 0.05091\tnstep_dev_loss: 0.09870\tloop_dev_ref_loss: 0.05039\tloop_dev_loss: nan\teltime:  31.34408\n",
      "epoch: 183\tnstep_dev_ref_loss: 0.05060\tnstep_dev_loss: 0.09843\tloop_dev_ref_loss: 0.05008\tloop_dev_loss: nan\teltime:  31.48991\n",
      "epoch: 184\tnstep_dev_ref_loss: 0.05025\tnstep_dev_loss: 0.09801\tloop_dev_ref_loss: 0.04971\tloop_dev_loss: nan\teltime:  31.63643\n",
      "epoch: 185\tnstep_dev_ref_loss: 0.04999\tnstep_dev_loss: 0.09792\tloop_dev_ref_loss: 0.04945\tloop_dev_loss: nan\teltime:  31.77305\n",
      "epoch: 186\tnstep_dev_ref_loss: 0.04967\tnstep_dev_loss: 0.09751\tloop_dev_ref_loss: 0.04912\tloop_dev_loss: nan\teltime:  31.91751\n",
      "epoch: 187\tnstep_dev_ref_loss: 0.04940\tnstep_dev_loss: 0.09732\tloop_dev_ref_loss: 0.04885\tloop_dev_loss: nan\teltime:  32.06126\n",
      "epoch: 188\tnstep_dev_ref_loss: 0.04913\tnstep_dev_loss: 0.09708\tloop_dev_ref_loss: 0.04857\tloop_dev_loss: nan\teltime:  32.20402\n",
      "epoch: 189\tnstep_dev_ref_loss: 0.04885\tnstep_dev_loss: 0.09684\tloop_dev_ref_loss: 0.04828\tloop_dev_loss: nan\teltime:  32.34810\n",
      "epoch: 190\tnstep_dev_ref_loss: 0.04857\tnstep_dev_loss: 0.09669\tloop_dev_ref_loss: 0.04799\tloop_dev_loss: nan\teltime:  32.50609\n",
      "epoch: 191\tnstep_dev_ref_loss: 0.04822\tnstep_dev_loss: 0.09635\tloop_dev_ref_loss: 0.04763\tloop_dev_loss: nan\teltime:  32.65730\n",
      "epoch: 192\tnstep_dev_ref_loss: 0.04792\tnstep_dev_loss: 0.09627\tloop_dev_ref_loss: 0.04734\tloop_dev_loss: nan\teltime:  32.80440\n",
      "epoch: 193\tnstep_dev_ref_loss: 0.04756\tnstep_dev_loss: 0.09588\tloop_dev_ref_loss: 0.04696\tloop_dev_loss: nan\teltime:  32.94230\n",
      "epoch: 194\tnstep_dev_ref_loss: 0.04729\tnstep_dev_loss: 0.09584\tloop_dev_ref_loss: 0.04669\tloop_dev_loss: nan\teltime:  33.08502\n",
      "epoch: 195\tnstep_dev_ref_loss: 0.04692\tnstep_dev_loss: 0.09537\tloop_dev_ref_loss: 0.04630\tloop_dev_loss: nan\teltime:  33.23130\n",
      "epoch: 196\tnstep_dev_ref_loss: 0.04665\tnstep_dev_loss: 0.09540\tloop_dev_ref_loss: 0.04603\tloop_dev_loss: nan\teltime:  33.38035\n",
      "epoch: 197\tnstep_dev_ref_loss: 0.04624\tnstep_dev_loss: 0.09479\tloop_dev_ref_loss: 0.04560\tloop_dev_loss: nan\teltime:  33.52565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 198\tnstep_dev_ref_loss: 0.04600\tnstep_dev_loss: 0.09496\tloop_dev_ref_loss: 0.04537\tloop_dev_loss: nan\teltime:  33.67400\n",
      "epoch: 199\tnstep_dev_ref_loss: 0.04557\tnstep_dev_loss: 0.09418\tloop_dev_ref_loss: 0.04491\tloop_dev_loss: nan\teltime:  33.82111\n",
      "epoch: 200\tnstep_dev_ref_loss: 0.04548\tnstep_dev_loss: 0.09489\tloop_dev_ref_loss: 0.04484\tloop_dev_loss: nan\teltime:  33.96721\n",
      "epoch: 201\tnstep_dev_ref_loss: 0.04491\tnstep_dev_loss: 0.09350\tloop_dev_ref_loss: 0.04422\tloop_dev_loss: nan\teltime:  34.11034\n",
      "epoch: 202\tnstep_dev_ref_loss: 0.04502\tnstep_dev_loss: 0.09522\tloop_dev_ref_loss: 0.04439\tloop_dev_loss: nan\teltime:  34.25365\n",
      "epoch: 203\tnstep_dev_ref_loss: 0.04406\tnstep_dev_loss: 0.09204\tloop_dev_ref_loss: 0.04332\tloop_dev_loss: nan\teltime:  34.41330\n",
      "epoch: 204\tnstep_dev_ref_loss: 0.04479\tnstep_dev_loss: 0.09629\tloop_dev_ref_loss: 0.04419\tloop_dev_loss: nan\teltime:  34.57280\n",
      "epoch: 205\tnstep_dev_ref_loss: 0.04322\tnstep_dev_loss: 0.08995\tloop_dev_ref_loss: 0.04241\tloop_dev_loss: nan\teltime:  34.71113\n",
      "epoch: 206\tnstep_dev_ref_loss: 0.04426\tnstep_dev_loss: 0.09577\tloop_dev_ref_loss: 0.04365\tloop_dev_loss: nan\teltime:  34.86463\n",
      "epoch: 207\tnstep_dev_ref_loss: 0.04271\tnstep_dev_loss: 0.08988\tloop_dev_ref_loss: 0.04192\tloop_dev_loss: nan\teltime:  35.00852\n",
      "epoch: 208\tnstep_dev_ref_loss: 0.04259\tnstep_dev_loss: 0.09030\tloop_dev_ref_loss: 0.04181\tloop_dev_loss: nan\teltime:  35.14931\n",
      "epoch: 209\tnstep_dev_ref_loss: 0.04301\tnstep_dev_loss: 0.09312\tloop_dev_ref_loss: 0.04233\tloop_dev_loss: nan\teltime:  35.29204\n",
      "epoch: 210\tnstep_dev_ref_loss: 0.04194\tnstep_dev_loss: 0.08864\tloop_dev_ref_loss: 0.04111\tloop_dev_loss: nan\teltime:  35.44253\n",
      "epoch: 211\tnstep_dev_ref_loss: 0.04208\tnstep_dev_loss: 0.09065\tloop_dev_ref_loss: 0.04131\tloop_dev_loss: nan\teltime:  35.58286\n",
      "epoch: 212\tnstep_dev_ref_loss: 0.04211\tnstep_dev_loss: 0.09174\tloop_dev_ref_loss: 0.04137\tloop_dev_loss: nan\teltime:  35.73140\n",
      "epoch: 213\tnstep_dev_ref_loss: 0.04127\tnstep_dev_loss: 0.08826\tloop_dev_ref_loss: 0.04042\tloop_dev_loss: nan\teltime:  35.88135\n",
      "epoch: 214\tnstep_dev_ref_loss: 0.04137\tnstep_dev_loss: 0.09033\tloop_dev_ref_loss: 0.04059\tloop_dev_loss: nan\teltime:  36.02561\n",
      "epoch: 215\tnstep_dev_ref_loss: 0.04116\tnstep_dev_loss: 0.09041\tloop_dev_ref_loss: 0.04039\tloop_dev_loss: nan\teltime:  36.16597\n",
      "epoch: 216\tnstep_dev_ref_loss: 0.04048\tnstep_dev_loss: 0.08786\tloop_dev_ref_loss: 0.03962\tloop_dev_loss: nan\teltime:  36.30249\n",
      "epoch: 217\tnstep_dev_ref_loss: 0.04061\tnstep_dev_loss: 0.09023\tloop_dev_ref_loss: 0.03983\tloop_dev_loss: nan\teltime:  36.44908\n",
      "epoch: 218\tnstep_dev_ref_loss: 0.04027\tnstep_dev_loss: 0.08988\tloop_dev_ref_loss: 0.03947\tloop_dev_loss: nan\teltime:  36.59957\n",
      "epoch: 219\tnstep_dev_ref_loss: 0.03969\tnstep_dev_loss: 0.08823\tloop_dev_ref_loss: 0.03884\tloop_dev_loss: nan\teltime:  36.74914\n",
      "epoch: 220\tnstep_dev_ref_loss: 0.04003\tnstep_dev_loss: 0.09112\tloop_dev_ref_loss: 0.03926\tloop_dev_loss: nan\teltime:  36.89516\n",
      "epoch: 221\tnstep_dev_ref_loss: 0.03957\tnstep_dev_loss: 0.09008\tloop_dev_ref_loss: 0.03876\tloop_dev_loss: nan\teltime:  37.04468\n",
      "epoch: 222\tnstep_dev_ref_loss: 0.03918\tnstep_dev_loss: 0.08930\tloop_dev_ref_loss: 0.03835\tloop_dev_loss: nan\teltime:  37.18671\n",
      "epoch: 223\tnstep_dev_ref_loss: 0.03955\tnstep_dev_loss: 0.09194\tloop_dev_ref_loss: 0.03880\tloop_dev_loss: nan\teltime:  37.34323\n",
      "epoch: 224\tnstep_dev_ref_loss: 0.03884\tnstep_dev_loss: 0.08975\tloop_dev_ref_loss: 0.03802\tloop_dev_loss: nan\teltime:  37.49107\n",
      "epoch: 225\tnstep_dev_ref_loss: 0.03874\tnstep_dev_loss: 0.09028\tloop_dev_ref_loss: 0.03794\tloop_dev_loss: nan\teltime:  37.63130\n",
      "epoch: 226\tnstep_dev_ref_loss: 0.03884\tnstep_dev_loss: 0.09161\tloop_dev_ref_loss: 0.03807\tloop_dev_loss: nan\teltime:  37.78572\n",
      "epoch: 227\tnstep_dev_ref_loss: 0.03812\tnstep_dev_loss: 0.08940\tloop_dev_ref_loss: 0.03728\tloop_dev_loss: nan\teltime:  37.93532\n",
      "epoch: 228\tnstep_dev_ref_loss: 0.03834\tnstep_dev_loss: 0.09134\tloop_dev_ref_loss: 0.03756\tloop_dev_loss: nan\teltime:  38.09006\n",
      "epoch: 229\tnstep_dev_ref_loss: 0.03794\tnstep_dev_loss: 0.09060\tloop_dev_ref_loss: 0.03715\tloop_dev_loss: nan\teltime:  38.22625\n",
      "epoch: 230\tnstep_dev_ref_loss: 0.03755\tnstep_dev_loss: 0.08981\tloop_dev_ref_loss: 0.03673\tloop_dev_loss: nan\teltime:  38.37542\n",
      "epoch: 231\tnstep_dev_ref_loss: 0.03776\tnstep_dev_loss: 0.09161\tloop_dev_ref_loss: 0.03699\tloop_dev_loss: nan\teltime:  38.52794\n",
      "epoch: 232\tnstep_dev_ref_loss: 0.03707\tnstep_dev_loss: 0.08955\tloop_dev_ref_loss: 0.03623\tloop_dev_loss: nan\teltime:  38.68129\n",
      "epoch: 233\tnstep_dev_ref_loss: 0.03708\tnstep_dev_loss: 0.09053\tloop_dev_ref_loss: 0.03627\tloop_dev_loss: nan\teltime:  38.83226\n",
      "epoch: 234\tnstep_dev_ref_loss: 0.03683\tnstep_dev_loss: 0.09034\tloop_dev_ref_loss: 0.03601\tloop_dev_loss: nan\teltime:  38.98858\n",
      "epoch: 235\tnstep_dev_ref_loss: 0.03637\tnstep_dev_loss: 0.08922\tloop_dev_ref_loss: 0.03551\tloop_dev_loss: nan\teltime:  39.14367\n",
      "epoch: 236\tnstep_dev_ref_loss: 0.03649\tnstep_dev_loss: 0.09073\tloop_dev_ref_loss: 0.03567\tloop_dev_loss: nan\teltime:  39.29910\n",
      "epoch: 237\tnstep_dev_ref_loss: 0.03590\tnstep_dev_loss: 0.08910\tloop_dev_ref_loss: 0.03503\tloop_dev_loss: nan\teltime:  39.44893\n",
      "epoch: 238\tnstep_dev_ref_loss: 0.03586\tnstep_dev_loss: 0.09001\tloop_dev_ref_loss: 0.03502\tloop_dev_loss: nan\teltime:  39.61347\n",
      "epoch: 239\tnstep_dev_ref_loss: 0.03562\tnstep_dev_loss: 0.08995\tloop_dev_ref_loss: 0.03476\tloop_dev_loss: nan\teltime:  39.77375\n",
      "epoch: 240\tnstep_dev_ref_loss: 0.03523\tnstep_dev_loss: 0.08926\tloop_dev_ref_loss: 0.03434\tloop_dev_loss: nan\teltime:  39.93785\n",
      "epoch: 241\tnstep_dev_ref_loss: 0.03527\tnstep_dev_loss: 0.09056\tloop_dev_ref_loss: 0.03442\tloop_dev_loss: nan\teltime:  40.08825\n",
      "epoch: 242\tnstep_dev_ref_loss: 0.03473\tnstep_dev_loss: 0.08910\tloop_dev_ref_loss: 0.03382\tloop_dev_loss: nan\teltime:  40.25088\n",
      "epoch: 243\tnstep_dev_ref_loss: 0.03483\tnstep_dev_loss: 0.09061\tloop_dev_ref_loss: 0.03397\tloop_dev_loss: nan\teltime:  40.39146\n",
      "epoch: 244\tnstep_dev_ref_loss: 0.03437\tnstep_dev_loss: 0.08952\tloop_dev_ref_loss: 0.03346\tloop_dev_loss: nan\teltime:  40.54839\n",
      "epoch: 245\tnstep_dev_ref_loss: 0.03432\tnstep_dev_loss: 0.09037\tloop_dev_ref_loss: 0.03344\tloop_dev_loss: nan\teltime:  40.68867\n",
      "epoch: 246\tnstep_dev_ref_loss: 0.03407\tnstep_dev_loss: 0.09022\tloop_dev_ref_loss: 0.03318\tloop_dev_loss: nan\teltime:  40.84176\n",
      "epoch: 247\tnstep_dev_ref_loss: 0.03386\tnstep_dev_loss: 0.09028\tloop_dev_ref_loss: 0.03296\tloop_dev_loss: nan\teltime:  40.99611\n",
      "epoch: 248\tnstep_dev_ref_loss: 0.03379\tnstep_dev_loss: 0.09099\tloop_dev_ref_loss: 0.03291\tloop_dev_loss: nan\teltime:  41.14210\n",
      "epoch: 249\tnstep_dev_ref_loss: 0.03339\tnstep_dev_loss: 0.09019\tloop_dev_ref_loss: 0.03248\tloop_dev_loss: nan\teltime:  41.30041\n",
      "epoch: 250\tnstep_dev_ref_loss: 0.03352\tnstep_dev_loss: 0.09176\tloop_dev_ref_loss: 0.03265\tloop_dev_loss: nan\teltime:  41.44893\n",
      "epoch: 251\tnstep_dev_ref_loss: 0.03289\tnstep_dev_loss: 0.08981\tloop_dev_ref_loss: 0.03195\tloop_dev_loss: nan\teltime:  41.59950\n",
      "epoch: 252\tnstep_dev_ref_loss: 0.03339\tnstep_dev_loss: 0.09294\tloop_dev_ref_loss: 0.03255\tloop_dev_loss: nan\teltime:  41.74443\n",
      "epoch: 253\tnstep_dev_ref_loss: 0.03226\tnstep_dev_loss: 0.08835\tloop_dev_ref_loss: 0.03127\tloop_dev_loss: nan\teltime:  41.90097\n",
      "epoch: 254\tnstep_dev_ref_loss: 0.03360\tnstep_dev_loss: 0.09489\tloop_dev_ref_loss: 0.03282\tloop_dev_loss: nan\teltime:  42.05433\n",
      "epoch: 255\tnstep_dev_ref_loss: 0.03160\tnstep_dev_loss: 0.08490\tloop_dev_ref_loss: 0.03050\tloop_dev_loss: nan\teltime:  42.20020\n",
      "epoch: 256\tnstep_dev_ref_loss: 0.03318\tnstep_dev_loss: 0.09313\tloop_dev_ref_loss: 0.03237\tloop_dev_loss: nan\teltime:  42.34443\n",
      "epoch: 257\tnstep_dev_ref_loss: 0.03121\tnstep_dev_loss: 0.08289\tloop_dev_ref_loss: 0.03014\tloop_dev_loss: nan\teltime:  42.49124\n",
      "epoch: 258\tnstep_dev_ref_loss: 0.03117\tnstep_dev_loss: 0.08225\tloop_dev_ref_loss: 0.03011\tloop_dev_loss: nan\teltime:  42.64516\n",
      "epoch: 259\tnstep_dev_ref_loss: 0.03182\tnstep_dev_loss: 0.08620\tloop_dev_ref_loss: 0.03088\tloop_dev_loss: nan\teltime:  42.78892\n",
      "epoch: 260\tnstep_dev_ref_loss: 0.03098\tnstep_dev_loss: 0.08057\tloop_dev_ref_loss: 0.02991\tloop_dev_loss: nan\teltime:  42.95489\n",
      "epoch: 261\tnstep_dev_ref_loss: 0.03092\tnstep_dev_loss: 0.07980\tloop_dev_ref_loss: 0.02984\tloop_dev_loss: nan\teltime:  43.10995\n",
      "epoch: 262\tnstep_dev_ref_loss: 0.03131\tnstep_dev_loss: 0.08355\tloop_dev_ref_loss: 0.03033\tloop_dev_loss: nan\teltime:  43.26796\n",
      "epoch: 263\tnstep_dev_ref_loss: 0.03081\tnstep_dev_loss: 0.08080\tloop_dev_ref_loss: 0.02977\tloop_dev_loss: nan\teltime:  43.41635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 264\tnstep_dev_ref_loss: 0.03053\tnstep_dev_loss: 0.07847\tloop_dev_ref_loss: 0.02944\tloop_dev_loss: nan\teltime:  43.56592\n",
      "epoch: 265\tnstep_dev_ref_loss: 0.03057\tnstep_dev_loss: 0.08125\tloop_dev_ref_loss: 0.02956\tloop_dev_loss: nan\teltime:  43.71520\n",
      "epoch: 266\tnstep_dev_ref_loss: 0.03044\tnstep_dev_loss: 0.08161\tloop_dev_ref_loss: 0.02944\tloop_dev_loss: nan\teltime:  43.86640\n",
      "epoch: 267\tnstep_dev_ref_loss: 0.02997\tnstep_dev_loss: 0.07885\tloop_dev_ref_loss: 0.02889\tloop_dev_loss: nan\teltime:  44.03018\n",
      "epoch: 268\tnstep_dev_ref_loss: 0.02988\tnstep_dev_loss: 0.08043\tloop_dev_ref_loss: 0.02884\tloop_dev_loss: nan\teltime:  44.17952\n",
      "epoch: 269\tnstep_dev_ref_loss: 0.03010\tnstep_dev_loss: 0.08323\tloop_dev_ref_loss: 0.02913\tloop_dev_loss: nan\teltime:  44.32133\n",
      "epoch: 270\tnstep_dev_ref_loss: 0.02958\tnstep_dev_loss: 0.08129\tloop_dev_ref_loss: 0.02854\tloop_dev_loss: nan\teltime:  44.47390\n",
      "epoch: 271\tnstep_dev_ref_loss: 0.02941\tnstep_dev_loss: 0.08163\tloop_dev_ref_loss: 0.02838\tloop_dev_loss: nan\teltime:  44.63353\n",
      "epoch: 272\tnstep_dev_ref_loss: 0.02981\tnstep_dev_loss: 0.08509\tloop_dev_ref_loss: 0.02887\tloop_dev_loss: nan\teltime:  44.78223\n",
      "epoch: 273\tnstep_dev_ref_loss: 0.02940\tnstep_dev_loss: 0.08394\tloop_dev_ref_loss: 0.02842\tloop_dev_loss: nan\teltime:  44.93251\n",
      "epoch: 274\tnstep_dev_ref_loss: 0.02907\tnstep_dev_loss: 0.08301\tloop_dev_ref_loss: 0.02807\tloop_dev_loss: nan\teltime:  45.07934\n",
      "epoch: 275\tnstep_dev_ref_loss: 0.02950\tnstep_dev_loss: 0.08601\tloop_dev_ref_loss: 0.02858\tloop_dev_loss: nan\teltime:  45.22458\n",
      "epoch: 276\tnstep_dev_ref_loss: 0.02923\tnstep_dev_loss: 0.08533\tloop_dev_ref_loss: 0.02830\tloop_dev_loss: nan\teltime:  45.36966\n",
      "epoch: 277\tnstep_dev_ref_loss: 0.02881\tnstep_dev_loss: 0.08378\tloop_dev_ref_loss: 0.02784\tloop_dev_loss: nan\teltime:  45.52349\n",
      "epoch: 278\tnstep_dev_ref_loss: 0.02919\tnstep_dev_loss: 0.08626\tloop_dev_ref_loss: 0.02829\tloop_dev_loss: nan\teltime:  45.66988\n",
      "epoch: 279\tnstep_dev_ref_loss: 0.02899\tnstep_dev_loss: 0.08589\tloop_dev_ref_loss: 0.02808\tloop_dev_loss: nan\teltime:  45.82085\n",
      "epoch: 280\tnstep_dev_ref_loss: 0.02851\tnstep_dev_loss: 0.08417\tloop_dev_ref_loss: 0.02757\tloop_dev_loss: nan\teltime:  45.96920\n",
      "epoch: 281\tnstep_dev_ref_loss: 0.02878\tnstep_dev_loss: 0.08604\tloop_dev_ref_loss: 0.02789\tloop_dev_loss: nan\teltime:  46.11888\n",
      "epoch: 282\tnstep_dev_ref_loss: 0.02856\tnstep_dev_loss: 0.08560\tloop_dev_ref_loss: 0.02766\tloop_dev_loss: nan\teltime:  46.26726\n",
      "epoch: 283\tnstep_dev_ref_loss: 0.02807\tnstep_dev_loss: 0.08376\tloop_dev_ref_loss: 0.02713\tloop_dev_loss: nan\teltime:  46.42064\n",
      "epoch: 284\tnstep_dev_ref_loss: 0.02821\tnstep_dev_loss: 0.08504\tloop_dev_ref_loss: 0.02731\tloop_dev_loss: nan\teltime:  46.57372\n",
      "epoch: 285\tnstep_dev_ref_loss: 0.02802\tnstep_dev_loss: 0.08468\tloop_dev_ref_loss: 0.02711\tloop_dev_loss: nan\teltime:  46.71489\n",
      "epoch: 286\tnstep_dev_ref_loss: 0.02758\tnstep_dev_loss: 0.08306\tloop_dev_ref_loss: 0.02662\tloop_dev_loss: nan\teltime:  46.86704\n",
      "epoch: 287\tnstep_dev_ref_loss: 0.02765\tnstep_dev_loss: 0.08414\tloop_dev_ref_loss: 0.02672\tloop_dev_loss: nan\teltime:  47.01605\n",
      "epoch: 288\tnstep_dev_ref_loss: 0.02750\tnstep_dev_loss: 0.08415\tloop_dev_ref_loss: 0.02658\tloop_dev_loss: nan\teltime:  47.17017\n",
      "epoch: 289\tnstep_dev_ref_loss: 0.02712\tnstep_dev_loss: 0.08293\tloop_dev_ref_loss: 0.02615\tloop_dev_loss: nan\teltime:  47.31236\n",
      "epoch: 290\tnstep_dev_ref_loss: 0.02715\tnstep_dev_loss: 0.08400\tloop_dev_ref_loss: 0.02621\tloop_dev_loss: nan\teltime:  47.46843\n",
      "epoch: 291\tnstep_dev_ref_loss: 0.02702\tnstep_dev_loss: 0.08417\tloop_dev_ref_loss: 0.02608\tloop_dev_loss: nan\teltime:  47.61819\n",
      "epoch: 292\tnstep_dev_ref_loss: 0.02667\tnstep_dev_loss: 0.08320\tloop_dev_ref_loss: 0.02570\tloop_dev_loss: nan\teltime:  47.77592\n",
      "epoch: 293\tnstep_dev_ref_loss: 0.02670\tnstep_dev_loss: 0.08425\tloop_dev_ref_loss: 0.02576\tloop_dev_loss: nan\teltime:  47.92029\n",
      "epoch: 294\tnstep_dev_ref_loss: 0.02658\tnstep_dev_loss: 0.08442\tloop_dev_ref_loss: 0.02564\tloop_dev_loss: nan\teltime:  48.07148\n",
      "epoch: 295\tnstep_dev_ref_loss: 0.02631\tnstep_dev_loss: 0.08375\tloop_dev_ref_loss: 0.02534\tloop_dev_loss: nan\teltime:  48.22484\n",
      "epoch: 296\tnstep_dev_ref_loss: 0.02637\tnstep_dev_loss: 0.08491\tloop_dev_ref_loss: 0.02543\tloop_dev_loss: nan\teltime:  48.36371\n",
      "epoch: 297\tnstep_dev_ref_loss: 0.02623\tnstep_dev_loss: 0.08496\tloop_dev_ref_loss: 0.02528\tloop_dev_loss: nan\teltime:  48.51923\n",
      "epoch: 298\tnstep_dev_ref_loss: 0.02602\tnstep_dev_loss: 0.08463\tloop_dev_ref_loss: 0.02506\tloop_dev_loss: nan\teltime:  48.65773\n",
      "epoch: 299\tnstep_dev_ref_loss: 0.02611\tnstep_dev_loss: 0.08585\tloop_dev_ref_loss: 0.02518\tloop_dev_loss: nan\teltime:  48.80502\n"
     ]
    }
   ],
   "source": [
    "best_model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: KeyError: 'loop_dev_loss'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Once training has concluded and the best model has been returned by the trainer, we can call the `evaluate()` method with the best model to evaluate the model's $N$-step and open-loop performance and use the visualizer to generate trace plots for both.\n",
    "\n",
    "Below, the model's $N$-step traces (top) and open-loop (bottom) traces are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tuor369/anaconda3/envs/neuromancer/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/home/tuor369/anaconda3/envs/neuromancer/lib/python3.8/site-packages/torch/nn/modules/container.py:435: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 299\tbest_loop_dev_loss: nan\teltime:  279.27509\n"
     ]
    }
   ],
   "source": [
    "best_outputs = trainer.test(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tuor369/anaconda3/envs/neuromancer/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "logger.clean_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
